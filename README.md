# Testbox - On-Premise & AWS Migration Testing Platform

A comprehensive learning project for testing on-premise services and AWS cloud migration strategies. This project implements a simple Todo List application with a focus on backend architecture, caching strategies, and message queuing.

## ğŸ¯ Project Purpose

- **Educational**: Learn and experiment with various on-premise and AWS services
- **MVP**: Simple Todo CRUD operations with title and content
- **Migration Ready**: Designed for easy migration from on-premise to AWS services
- **Scalable**: Architecture supports load balancing, Kubernetes, and MongoDB sharding

## ğŸ—ï¸ Architecture

### Current Stack (On-Premise)

**Backend:**
- Go 1.22 with Fiber v2 (HTTP framework)
- GORM (ORM)
- PostgreSQL (Primary database)
- Redis (Caching layer with Lazy Loading + Write-Through strategies)
- RabbitMQ (Message queue for async operations)
- MongoDB (Prepared for future replicaset + sharding tests)

**Frontend:**
- Vanilla HTML, CSS, JavaScript
- Nginx as web server and reverse proxy

**Infrastructure:**
- Docker & Docker Compose
- Ready for Kubernetes migration

### Future Migration Path (AWS)

- **PostgreSQL** â†’ **DynamoDB**
- **RabbitMQ** â†’ **SQS**
- **Fiber Backend** â†’ **Elastic Beanstalk** or **ECS/EKS**
- **Load Balancing** â†’ **ALB + Auto Scaling Groups**
- **Compute** â†’ **EC2** or **ECS/EKS**

## ğŸ“ Project Structure

```
testbox/
â”œâ”€â”€ backend/                 # Go/Fiber backend
â”‚   â”œâ”€â”€ cmd/
â”‚   â”‚   â””â”€â”€ main.go         # Application entry point
â”‚   â”œâ”€â”€ internal/
â”‚   â”‚   â”œâ”€â”€ api/            # HTTP handlers and routes
â”‚   â”‚   â”œâ”€â”€ cache/          # Redis caching implementation
â”‚   â”‚   â”œâ”€â”€ config/         # Configuration management
â”‚   â”‚   â”œâ”€â”€ database/       # Database connections
â”‚   â”‚   â”œâ”€â”€ messaging/      # RabbitMQ integration
â”‚   â”‚   â”œâ”€â”€ models/         # Data models
â”‚   â”‚   â”œâ”€â”€ repository/     # Data access layer
â”‚   â”‚   â””â”€â”€ service/        # Business logic layer
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ go.mod
â”‚   â””â”€â”€ go.sum
â”œâ”€â”€ frontend/               # Vanilla JS frontend
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â””â”€â”€ app.js
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ docker-compose.yml      # Multi-container orchestration
â”œâ”€â”€ nginx.conf             # Nginx configuration
â”œâ”€â”€ Makefile               # Development shortcuts
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- Go 1.22+ (for local development)
- Make (optional, for convenience)

### Running with Docker (Recommended)

1. **Clone and navigate to the project:**
   ```bash
   cd testbox
   ```

2. **Start all services:**
   ```bash
   make up
   # or
   docker-compose up -d
   ```

3. **Access the application:**
   - Frontend: http://localhost:8080
   - Backend API: http://localhost:3000/api/todos
   - Health Check: http://localhost:3000/health
   - RabbitMQ Management: http://localhost:15672 (guest/guest)

4. **View logs:**
   ```bash
   make logs
   # or
   docker-compose logs -f
   ```

5. **Stop services:**
   ```bash
   make down
   # or
   docker-compose down
   ```

### Local Development (Backend)

1. **Start infrastructure services:**
   ```bash
   docker-compose up -d postgres redis rabbitmq mongodb
   ```

2. **Run backend:**
   ```bash
   cd backend
   go mod download
   go run cmd/main.go
   ```

3. **Serve frontend:**
   ```bash
   cd frontend
   python3 -m http.server 8080
   ```

## ğŸ”§ Configuration

Environment variables can be set in `.env` file (see `.env.example`):

```env
SERVER_PORT=3000
DB_HOST=localhost
DB_PORT=5432
REDIS_HOST=localhost
REDIS_PORT=6379
RABBITMQ_URL=amqp://guest:guest@localhost:5672/
MONGO_URL=mongodb://localhost:27017
```

## ğŸ“š Key Learning Points

### 1. Caching Strategies

**Lazy Loading (Cache-Aside):**
- Used in `GetTodo()` operation
- Check cache first â†’ on miss, fetch from DB â†’ populate cache
- Benefits: Only caches what's needed

**Write-Through:**
- Used in `CreateTodo()` and `UpdateTodo()` operations
- Write to DB first â†’ immediately write to cache
- Benefits: Cache is always up-to-date

Implementation: [backend/internal/cache/redis.go](backend/internal/cache/redis.go)

### 2. Layered Architecture

```
Handler (API) â†’ Service (Business Logic) â†’ Repository (Data Access)
                    â†“                           â†“
                  Cache                      Database
                    â†“
                Messaging
```

### 3. Message Queue Integration

- Async event publishing for todo operations
- Prepared for migration to AWS SQS
- Implementation: [backend/internal/messaging/rabbitmq.go](backend/internal/messaging/rabbitmq.go)

### 4. Database Abstraction

- Repository pattern for easy database switching
- GORM for ORM operations
- Ready for DynamoDB migration

## ğŸ” API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/todos` | Get all todos |
| GET | `/api/todos/:id` | Get a specific todo |
| POST | `/api/todos` | Create a new todo |
| PUT | `/api/todos/:id` | Update a todo |
| DELETE | `/api/todos/:id` | Delete a todo |
| GET | `/health` | Health check |

### Example Requests

**Create Todo:**
```bash
curl -X POST http://localhost:3000/api/todos \
  -H "Content-Type: application/json" \
  -d '{"title":"Learn Go","content":"Study Fiber v3 framework"}'
```

**Get All Todos:**
```bash
curl http://localhost:3000/api/todos
```

**Update Todo:**
```bash
curl -X PUT http://localhost:3000/api/todos/{id} \
  -H "Content-Type: application/json" \
  -d '{"title":"Learn Go","content":"Completed!","completed":true}'
```

## ğŸ“ Learning Roadmap

### Phase 1: On-Premise (Current)
- âœ… Go/Fiber backend with GORM
- âœ… PostgreSQL database
- âœ… Redis caching (Lazy Loading + Write-Through)
- âœ… RabbitMQ message queue
- âœ… Docker containerization
- ğŸ”„ Kubernetes deployment (planned)
- ğŸ”„ MongoDB replicaset + sharding (planned)

### Phase 2: AWS Migration
- ğŸ”„ DynamoDB integration
- ğŸ”„ SQS message queue
- ğŸ”„ Elastic Beanstalk deployment
- ğŸ”„ ALB + Auto Scaling Groups
- ğŸ”„ ECS/EKS container orchestration

### Phase 3: Advanced Topics
- ğŸ”„ Multi-region deployment
- ğŸ”„ Blue-green deployment
- ğŸ”„ Canary releases
- ğŸ”„ Performance testing and optimization
- ğŸ”„ Observability (logging, monitoring, tracing)

## ğŸ› ï¸ Makefile Commands

```bash
make help           # Show all available commands
make up             # Start all services
make down           # Stop all services
make build          # Build all services
make logs           # Show logs
make clean          # Remove all containers and volumes
make restart        # Restart services
make health         # Check health of all services
make backend-dev    # Run backend in development mode
make frontend-dev   # Serve frontend locally
```

## ğŸ“Š Service Ports

| Service | Port | Description |
|---------|------|-------------|
| Frontend | 8080 | Nginx web server |
| Backend | 3000 | Go/Fiber API |
| PostgreSQL | 5432 | Database |
| Redis | 6379 | Cache |
| RabbitMQ | 5672 | Message queue |
| RabbitMQ UI | 15672 | Management interface |
| MongoDB | 27017 | Document database |

## ğŸ§ª Testing

```bash
# Backend tests
cd backend
go test -v ./...

# Health check
make health
```

## ğŸ“ Future Enhancements

- [ ] Add comprehensive unit and integration tests
- [ ] Implement MongoDB adapter alongside PostgreSQL
- [ ] Create Kubernetes manifests (deployments, services, ingress)
- [ ] Add CI/CD pipeline
- [ ] Implement AWS SDK for DynamoDB and SQS
- [ ] Add observability stack (Prometheus, Grafana)
- [ ] Implement authentication and authorization
- [ ] Add rate limiting and request throttling
- [ ] Create Terraform/CloudFormation templates for AWS

## ğŸ¤ Contributing

This is a personal learning project, but suggestions and improvements are welcome!

## ğŸ“„ License

MIT License - Feel free to use this project for learning purposes.

---

**Note:** This project is designed for learning and experimentation. Security features like authentication, input validation, and rate limiting should be enhanced before any production use.
